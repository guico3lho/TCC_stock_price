{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Packages and Assets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:29:55.768440Z",
     "start_time": "2023-05-23T14:29:55.638515100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "df_train_preprocessed = pd.read_csv('../../assets/data/splits/train/preprocessed.csv')\n",
    "\n",
    "df_val_preprocessed = pd.read_csv('../../assets/data/splits/val/preprocessed.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:29:55.799422700Z",
     "start_time": "2023-05-23T14:29:55.651508100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def createVocabulary(corpus):\n",
    "    \"\"\"\n",
    "    - Cria vocab (palavra e sua respectiva frequÃªncia no corpus)\n",
    "    - Cria tokens (palavras do corpus)\n",
    "\n",
    "    :param corpus: lista de string\n",
    "    :return vocab, tokens, vocab_size:\n",
    "    \"\"\"\n",
    "    tokens = []  # {'deeds', 'old', ...} 71666\n",
    "    vocab = {}  # {'deeds': 2, 'old': 20', ...} 17971\n",
    "    for text in corpus:\n",
    "        for token in text.split():\n",
    "            tokens.append(token)\n",
    "            if token in vocab:\n",
    "                vocab[token] += 1\n",
    "            else:\n",
    "                vocab[token] = 1\n",
    "    vocab_size = len(vocab)\n",
    "    return tokens, vocab, vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:29:55.809416Z",
     "start_time": "2023-05-23T14:29:55.768440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def findMaxLen(sequence):\n",
    "    max_len = 0\n",
    "    for text in sequence:\n",
    "        if len(text) > max_len:\n",
    "            max_len = len(text)\n",
    "    return max_len\n",
    "\n",
    "def findAverageLen(sequence):\n",
    "    total_len = 0\n",
    "    for text in sequence:\n",
    "        total_len += len(text)\n",
    "    return total_len / len(sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:29:55.809416Z",
     "start_time": "2023-05-23T14:29:55.773439Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Separating each split into features (X) and target (y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "X_train = df_train_preprocessed.title\n",
    "y_train = df_train_preprocessed.label\n",
    "\n",
    "X_val = df_val_preprocessed.title\n",
    "y_val = df_val_preprocessed.label\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:29:55.830405200Z",
     "start_time": "2023-05-23T14:29:55.799422700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating tokens and vocabulary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "2910"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens, vocab, vocab_size = createVocabulary(X_train)\n",
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:29:55.837400800Z",
     "start_time": "2023-05-23T14:29:55.803420200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Numericalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "val_sequences = tokenizer.texts_to_sequences(X_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:29:55.912356500Z",
     "start_time": "2023-05-23T14:29:55.836400900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = findMaxLen(train_sequences)\n",
    "max_len = int(max_len/2)\n",
    "max_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:45:03.397962400Z",
     "start_time": "2023-05-23T14:45:03.380961700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:45:07.661512700Z",
     "start_time": "2023-05-23T14:45:07.617924100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "train_padded = np.insert(train_padded, max_len, y_train, 1)\n",
    "val_padded = np.insert(val_padded, max_len, y_val, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:45:15.494512700Z",
     "start_time": "2023-05-23T14:45:15.470528Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Convert the tokenizer to a dictionary\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "\n",
    "# Save the tokenizer to a file\n",
    "with open('../../assets/deep_assets/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:45:22.214220800Z",
     "start_time": "2023-05-23T14:45:22.153009900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "pd.DataFrame(train_padded).to_csv('../../assets/data/splits/train/padded.csv', index=False)\n",
    "pd.DataFrame(val_padded).to_csv('../../assets/data/splits/val/padded.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T14:45:23.810243800Z",
     "start_time": "2023-05-23T14:45:23.734812800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
